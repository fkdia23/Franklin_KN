---
author: Franklin KANA NGUEDIA 
pubDatetime: 2025-12-13T16:52:45.934Z
modDatetime: 2025-12-13T16:52:45.934Z
title: Construire un RAG fiable, tra√ßable et auditable
slug: rag-precision-tracabilite
featured: true
draft: false
tags:
  - RAG
  - Ursaff Caisse Nationale
description: >
  Concevoir un syst√®me RAG capable de fournir des r√©ponses pr√©cises,
  v√©rifiables et sourc√©es √† partir de documents complexes, avec
  tra√ßabilit√© compl√®te jusqu‚Äô√† la page source.
---

Comment l'URSSAF peut r√©volutionner sa gestion documentaire avec un syst√®me RAG de haute pr√©cision
√âtude de cas : Transformer 60 ans d'archives r√©glementaires en assistant intelligent pour 45,000 collaborateurs

*¬´ J‚Äôai con√ßu un syst√®me RAG de haute pr√©cision avec tra√ßabilit√© visuelle, initialement pour des donn√©es techniques complexes. Je suis convaincu qu‚Äôil r√©pond √† un besoin critique de l‚ÄôURSSAF : permettre aux agents de s‚Äôappuyer sur une IA g√©n√©rative fiable, auditable et conforme pour interpr√©ter la r√©glementation sociale.

Ce prototype int√®gre d√©j√† les piliers demand√©s : ingestion de documents non structur√©s, retrieval pr√©cis avec m√©tadonn√©es (page, document), g√©n√©ration sourc√©e, monitoring des hallucinations, et feedback utilisateurs.

En tant que Data Engineer IA, je souhaite contribuer √† industrialiser ce type de solution au sein de la Fabrique, en collaboration avec les Data Scientists et les m√©tiers, pour transformer la donn√©e r√©glementaire en levier de qualit√© du service public. ¬ª*

D√©mo :
Code source : 

## Table of contents

## **1. Le D√©fi : Pourquoi les r√©ponses g√©n√©riques ne suffisvent pas pour des donn√©es techniques complexes**

- **Contexte** : Dans les domaines techniques (ing√©nierie, recherche, sant√©, droit), les utilisateurs ont besoin de **r√©ponses exactes**, souvent **sourc√©es** et **v√©rifiables**.
- **Limites des chatbots classiques** :
  - R√©ponses hallucin√©es ou trop g√©n√©riques
  - Aucune tra√ßabilit√© vers la source
  - Incapacit√© √† justifier une affirmation par un extrait pr√©cis
- **Cons√©quence** : Perte de confiance, inutilisabilit√© dans des contextes critiques (audit, support technique, recherche)
- **Objectif du projet** : Aller au-del√† de la simple g√©n√©ration ‚Üí **r√©pondre avec pr√©cision** (*precision*) **et prouvabilit√©** (*provenance*).


## **2. La Solution Prototype : Un RAG qui cite, surligne et renvoie √† la source**

üí° La Solution : Un RAG sur mesure pour l'URSSAF
Vision : L'assistant r√©glementaire intelligent
Imaginons CLARA (Conseiller L√©gislatif et R√©glementaire Augment√©), un syst√®me RAG pens√© sp√©cifiquement pour l'URSSAF.

- **Fonctionnalit√©s cl√©s du prototype** :
  - R√©ponse g√©n√©r√©e **avec citation explicite** : nom du document, auteur, section/page
  - **Surlignage dynamique** du passage source dans le texte original
  - **Deep linking** vers la page exacte du PDF source (ex: `#page=42`)
- **Exp√©rience utilisateur** : L‚Äôutilisateur voit non seulement *quoi*, mais aussi *d‚Äôo√π* vient l‚Äôinformation ‚Üí transparence totale.
- **Cas d‚Äôusage illustratif** : Exemple concret (ex: ing√©nieur cherchant une sp√©cification dans un manuel technique de 300 pages).

### 2.1 Workflow Utilisateur

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    1. INGESTION                                  ‚îÇ
‚îÇ  Utilisateur ‚Üí Upload Documents ‚Üí Traitement ‚Üí Neo4j             ‚îÇ
‚îÇ  (PDF, DOCX, TXT, URL)                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    2. EMBEDDING                                  ‚îÇ
‚îÇ  Texte ‚Üí Chunks ‚Üí Ollama (nomic-embed-text) ‚Üí Vecteurs         ‚îÇ
‚îÇ  Stockage avec m√©tadonn√©es (page, position)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    3. REQU√äTE                                    ‚îÇ
‚îÇ  Question ‚Üí Embedding ‚Üí Recherche similaire ‚Üí Top-K chunks      ‚îÇ
‚îÇ  Similarit√© cosine > 0.5                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    4. G√âN√âRATION                                 ‚îÇ
‚îÇ  Chunks + Question ‚Üí LLM (Mistral) ‚Üí R√©ponse contextuelle       ‚îÇ
‚îÇ  Avec citations et r√©f√©rences pr√©cises                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    5. PR√âSENTATION                               ‚îÇ
‚îÇ  R√©ponse + Sources (Fichier, Page, Texte surlign√©)             ‚îÇ
‚îÇ  Interface React interactive                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## **3. L‚ÄôArchitecture : Choix techniques pour la pr√©cision et la tra√ßabilit√©**

- **Strat√©gie de chunking** :
  - D√©coupage en **blocs de 500 caract√®res avec overlap de 50** pour pr√©server le contexte
  - Enrichissement syst√©matique par **m√©tadonn√©es** : nom du fichier, num√©ro de page, position dans le document
- **Base vectorielle** :
  - Utilisation de **Neo4j** (version 5.14+) pour sa capacit√© native √† combiner **graphe + embeddings**
  - Stockage des vecteurs dans des propri√©t√©s de n≈ìuds `Chunk`, avec index vectoriel optimis√© (`gds.similarity.cosine`)
- **Mod√®le d‚Äôembedding** :
  - `nomic-embed-text` (via Ollama) : 768 dimensions, multilingue, performant sur le retrieval
- **Deep Linking vers le PDF** :
  - Extraction du texte **page par page** avec `PyMuPDF`
  - G√©n√©ration d‚ÄôURL de la forme `document.pdf#page=XX` √† partir du num√©ro de page stock√© dans les m√©tadonn√©es
  - Restitution directe dans l‚Äôinterface utilisateur pour navigation instantan√©e

### 3.1 Stack Technique S√©lectionn√©e

- **Backend** : FastAPI (Python 3.11), asynchrone, avec validation Pydantic
- **LLM local** : Ollama (Mistral 7B pour g√©n√©ration, nomic-embed-text pour embeddings)
- **Stockage** : Neo4j pour documents, chunks, embeddings et relations
- **Frontend** : React 18 + TailwindCSS + PDF.js pour le surlignage et le deep linking
- **Infra** : Docker Compose (Neo4j, Ollama, Backend, Frontend, Prometheus, Grafana)
- **Gestion des d√©pendances** : `uv` (remplacement ultra-rapide de pip)

---


```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         COUCHE PR√âSENTATION                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  React 18 + TailwindCSS                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Chat Interface                                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Document Upload                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Source Reference Viewer                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üï HTTP/REST
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          COUCHE API                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  FastAPI (Python 3.11)                                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - REST Endpoints                                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Upload Management                                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Query Processing                                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Prometheus Metrics                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üï
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       COUCHE SERVICES                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  RAG Service    ‚îÇ  ‚îÇ Doc Processor   ‚îÇ  ‚îÇ  Neo4j Service   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Query        ‚îÇ  ‚îÇ - Extract       ‚îÇ  ‚îÇ  - Store         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Search       ‚îÇ  ‚îÇ - Chunk         ‚îÇ  ‚îÇ  - Search        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Generate     ‚îÇ  ‚îÇ - Embed         ‚îÇ  ‚îÇ  - Retrieve      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üï                    ‚Üï                    ‚Üï
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Ollama (LLM)   ‚îÇ  ‚îÇ  Neo4j (Graph)   ‚îÇ  ‚îÇ  Prometheus/Grafana  ‚îÇ
‚îÇ  - Mistral       ‚îÇ  ‚îÇ  - Documents     ‚îÇ  ‚îÇ  - Metrics           ‚îÇ
‚îÇ  - nomic-embed   ‚îÇ  ‚îÇ  - Chunks        ‚îÇ  ‚îÇ  - Dashboards        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 Stack Technique D√©taill√©e

#### Backend

```yaml
Language: Python 3.11+
Framework: FastAPI 0.104+
Package Manager: UV (ultra-fast pip alternative)

Core Libraries:
  - langchain: Orchestration RAG
  - neo4j: Driver base de donn√©es
  - ollama: Interface LLM local
  - pypdf: Extraction PDF
  - python-docx: Extraction Word
  - chardet: D√©tection encodage UTF-8
  - prometheus-client: M√©triques

Dependencies:
  - pydantic: Validation donn√©es
  - aiofiles: Async file operations
  - uvicorn: ASGI server
```

#### Frontend

```yaml
Framework: React 18
Build Tool: Vite 5
Styling: TailwindCSS 3
HTTP Client: Axios

Features:
  - Real-time chat interface
  - Drag & drop file upload
  - Source highlighting
  - Responsive design
```

#### Infrastructure

```yaml
Orchestration: Docker Compose
Containers:
  - Neo4j 5.14 (Community)
  - Ollama (Latest)
  - FastAPI Backend
  - React Frontend
  - Prometheus
  - Grafana

Storage:
  - neo4j_data: Graph persistence
  - ollama_data: Models cache
  - uploads: Document storage
```

### 3.3 Flux de Donn√©es

```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend
    participant A as API
    participant D as DocProcessor
    participant O as Ollama
    participant N as Neo4j
    
    U->>F: Upload document.pdf
    F->>A: POST /upload/file
    A->>D: process_document()
    D->>D: Extract text by page
    D->>D: Split into chunks (500 chars)
    
    loop For each chunk
        D->>O: generate_embedding()
        O-->>D: [vector 768 dims]
        D->>N: store_chunk_with_embedding()
    end
    
    A-->>F: {chunks_created: 45}
    F-->>U: Upload success
    
    Note over U,N: Later: User asks question
    
    U->>F: "Quelle est la politique?"
    F->>A: POST /query
    A->>O: generate_embedding(question)
    O-->>A: [query_vector]
    A->>N: similarity_search(vector, top_k=5)
    N-->>A: [top_chunks with metadata]
    A->>O: generate_answer(question, chunks)
    O-->>A: answer + reasoning
    A->>A: identify_used_chunks()
    A-->>F: {answer, sources, time}
    F-->>U: Display with references
```

---

## **4. Approche End-to-End : Du document brut √† l‚Äôinterface utilisateur**

Le syst√®me suit un pipeline lin√©aire et observable :

1. **Ingestion** : L‚Äôutilisateur d√©pose un document (PDF, DOCX, etc.). Le backend extrait le texte **page par page**.
2. **Pr√©traitement** : Le texte est d√©coup√© en chunks de 500 caract√®res (50 de chevauchement) et enrichi avec :
   - `filename`, `page_number`, `start_char`, `end_char`
3. **Vectorisation** : Chaque chunk est transform√© en vecteur via `nomic-embed-text` et stock√© dans Neo4j avec ses m√©tadonn√©es.
4. **Requ√™te** : √Ä chaque question, le syst√®me :
   - Convertit la requ√™te en vecteur
   - Interroge Neo4j pour les chunks les plus similaires (seuil cosine > 0.5)
   - R√©cup√®re les extraits + m√©tadonn√©es compl√®tes
5. **G√©n√©ration** : Le LLM (Mistral) re√ßoit la question + les chunks pertinents et g√©n√®re une r√©ponse **avec citations int√©gr√©es**.
6. **Pr√©sentation** : L‚Äôinterface affiche :
   - La r√©ponse synth√©tique
   - Les sources : nom du fichier, page, extrait surlign√©
   - Un lien cliquable (`#page=XX`) ouvrant directement la page du PDF

Cette cha√Æne garantit **reproductibilit√©**, **tra√ßabilit√©** et **exp√©rience utilisateur transparente**.
### 4.1. Justification des Choix Techniques

#### 4.1.1 Pourquoi Neo4j (Base de Donn√©es Graph) ?

**Avantages pour le RAG:**

1. **Relations Naturelles**
    ```cypher
	 (Document)-[:CONTAINS]->(Chunk)-[:RELATES_TO]->(Concept)
    ```
    - Mod√©lisation intuitive des relations document-chunk
    - Travers√©e rapide pour trouver documents connexes
    - Extension future : graphe de connaissances (entit√©s, concepts)
    
2. **Recherche Vectorielle Native**
    
    ```cypher
    // Neo4j 5.x supporte les embeddings
    MATCH (c:Chunk)
    WITH c, gds.similarity.cosine(c.embedding, $query_vector) AS score
    WHERE score > 0.5
    RETURN c ORDER BY score DESC
    ```
    
1. **Performance**
    - Index sur embeddings : recherche < 100ms
    - Cypher optimis√© pour graphes
    - Mieux que PostgreSQL + pgvector pour relations complexes

2. **√âvolutivit√©**
    - Ajout facile de n≈ìuds (Entities, Topics, Authors)
    - Requ√™tes multi-hop : "Documents similaires √† celui-ci"
    - Tra√ßabilit√© : historique des modifications

#### 4.1.2 Pourquoi Ollama (LLM Local) ?

**Raisons Strat√©giques:**

1. **Confidentialit√© et Conformit√©**
    - Donn√©es sensibles ne quittent jamais l'infrastructure
    - Conformit√© RGPD garantie
    - Pas de risque de fuite vers OpenAI/Anthropic

2. **Performance Pr√©visible**
    - Pas de rate limits
    - Latence constante (1-3s)
    - Pas de d√©pendance r√©seau
    
3. **Personnalisation**
    - Fine-tuning possible sur donn√©es m√©tier
    - Contr√¥le total des prompts
    - Support multilingue (fran√ßais natif)

**Mod√®les Choisis:**

- **Mistral 7B** pour g√©n√©ration
    - Excellent en fran√ßais
    - Performances comparables GPT-3.5
    - Rapide (2-3s sur GPU)
- **nomic-embed-text** pour embeddings
    - 768 dimensions
    - Optimis√© retrieval
    - Support multilingue

#### 4.1.3 Pourquoi FastAPI + UV ?

**FastAPI:**

- **Performance** : Async natif (3x plus rapide que Flask)
- **Documentation** : Swagger automatique
- **Typage** : Pydantic pour validation
- **WebSocket** : Support temps r√©el futur

**UV (Package Manager):**

```bash
# Comparaison vitesse installation
pip install -r requirements.txt    # 45s
poetry install                      # 35s
uv pip install -r requirements.txt  # 8s ‚úÖ
```

- **10x plus rapide** que pip
- R√©solution d√©pendances ultra-rapide
- Compatible pip (drop-in replacement)

### 4.5 Architecture de Chunking

**Strat√©gie Choisie:**

```python
CHUNK_SIZE = 500 caract√®res
CHUNK_OVERLAP = 50 caract√®res

Exemple:
Document: "Lorem ipsum... [2000 chars]"
‚Üí Chunk 1: chars 0-500
‚Üí Chunk 2: chars 450-950   # Overlap 50
‚Üí Chunk 3: chars 900-1400
‚Üí Chunk 4: chars 1350-1850
```

**Justification:**

|Taille Chunk|Avantages|Inconv√©nients|Verdict|
|---|---|---|---|
|100-200 chars|Pr√©cis|Perd contexte|‚ùå Trop petit|
|**500 chars**|‚úÖ **Balance**|-|‚úÖ **Optimal**|
|1000+ chars|Plus contexte|Bruit, lent|‚ùå Trop gros|

**Overlap:**

- 50 chars garantit pas de coupure phrases
- √âvite perte d'information aux fronti√®res
- Co√ªt: +10% stockage (acceptable)

### 4.6 Mod√®le de Donn√©es Neo4j

#### Sch√©ma Graphe

```cypher
// Node: Document
CREATE (d:Document {
  id: "md5_hash_filename",
  filename: "politique_remboursement.pdf",
  created_at: datetime(),
  file_size: 2048576,
  file_type: "pdf",
  page_count: 15
})

// Node: Chunk
CREATE (c:Chunk {
  id: "md5_filename_chunk_0",
  text: "Les remboursements sont...",
  page_number: 3,
  chunk_index: 0,
  start_char: 0,
  end_char: 500,
  embedding: [0.023, -0.156, ...],  // 768 dimensions
  char_count: 485,
  word_count: 72
})

// Relation
CREATE (d)-[:CONTAINS {
  order: 0,
  confidence: 1.0
}]->(c)
```

#### Index et Contraintes

```cypher
// Performance: index sur recherches fr√©quentes
CREATE INDEX chunk_id IF NOT EXISTS 
FOR (c:Chunk) ON (c.id);

CREATE INDEX doc_id IF NOT EXISTS 
FOR (d:Document) ON (d.id);

CREATE INDEX doc_filename IF NOT EXISTS 
FOR (d:Document) ON (d.filename);

// Full-text search backup
CREATE FULLTEXT INDEX chunk_text IF NOT EXISTS 
FOR (c:Chunk) ON EACH [c.text];

// Contraintes unicit√©
CREATE CONSTRAINT doc_id_unique IF NOT EXISTS
FOR (d:Document) REQUIRE d.id IS UNIQUE;

```

![alt text](image.png)

## 5. Am√©lioration : 

Un RAG n‚Äôest utile que s‚Äôil est **fiable dans la dur√©e**. La pr√©cision initiale ne suffit pas : le syst√®me doit **s‚Äôauto-√©valuer, s‚Äôadapter et gagner la confiance** de ses utilisateurs au fil du temps. Pour y parvenir, nous devons mettre en place des /
#### **M√©triques cl√©s √† surveiller en continu**

- **Latence par √©tape** :    
    - Ingestion (temps de traitement par document)
    - Retrieval (recherche vectorielle < 100 ms cible)
    - G√©n√©ration (temps de r√©ponse LLM)
    
- **Qualit√© du retrieval** :    
    - Pr√©cision et rappel √©valu√©s via **LLM-as-a-judge** (ex: ¬´ Ce chunk r√©pond-il √† la question ? ¬ª)
    - Comparaison avec un jeu de test m√©tier valid√© par des experts
    
- **Taux d‚Äôhallucination** :
    - V√©rification automatique que chaque affirmation dans la r√©ponse est **directement soutenue** par au moins un extrait source
    - D√©tection via NLI (Natural Language Inference) ou r√®gles de couverture lexicale/s√©mantique

#### **Observabilit√© op√©rationnelle**

- **Logs structur√©s** (format JSON) incluant :
    - Identifiant de requ√™te unique
    - Liste des chunks r√©cup√©r√©s (ID, score cosine, m√©tadonn√©es)
    - Prompt envoy√© au LLM et r√©ponse brute
    
- **Dashboard centralis√©** (Grafana + Prometheus) :
    - Taux d‚Äôerreur, volume de requ√™tes, distribution des latences
    - √âvolution du taux de succ√®s du retrieval sur 7/30 jours
    - Alertes en cas de d√©gradation soudaine 

## **Backend et s√©curit√©**

- **S√©curit√© des donn√©es** :
    - Isolation des documents par **tenant** (multi-tenant l√©ger via pr√©fixe dans les IDs)
    - Chiffrement des uploads au repos (AES-256) et en transit (TLS 1.3)
    - Pas de fuite vers des services externes : **100 % local** (Ollama + Neo4j en interne)

## 6. La d√©mo : 